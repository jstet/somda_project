\chapter{Data}
\label{sec:data}
This project aims to analyze the relationship between Wikipedia page views and turnout in the EU elections of 2009, 2014, and 2019. Included countries are the same as in the paper by Yasseri et al. To achieve this, two key metrics are required: Wikipedia page views for the relevant Wikipedia pages on the elections in the included countries, and the election turnout data. To obtain these metrics, the project retrieves data from two sources: Wikipedia Page View Dumps and the official EU website containing election results. The Wikipedia page views are accessed through downloadable files, as the API Wikimedia provides for this purpose only provides data from 2015 onwards. 

These files contain page views from all Wikimedia projects in all languages and can be quite large. In 2019, for example, they range from 400-500 MB uncompressed to around 4 GB uncompressed. The data before December 2011 has a different format both in content and on the time basis the files exist at. This necessitates separate data processing steps to handle these differences. Another challenge is that article names change over time and the page view files don't include something like an identifier that is consistent over time. To get the page names, I basically used the translation button on the English page for an election and then set the language to the relevant Wikipedia version. I considered implementing the MediaWiki Action API to find out whether article names changed and then to automatically correct the names, but this would also have been beyond the scope of this project. Unfortunately, the authors did not include the names of the articles with their paper, but they also write that there were difficulties in finding the names of the articles. One choice is between an election article on the general election and the article of the election in the corresponding country. Like the authors, I chose the article with the largest number of page views.

To process the page view dump files, the project uses the Python framework and cloud computing provider modal.com. Intermediate results are stored on a remote S3 server.  Files corresponding to 14 days before and after the election date are downloaded and processed concurrently, handling 100 files at a time. The time span of 28 days is chosen to limit the amount of data to be processed as it would have been too resource and time-consuming for the scope of this project otherwise. The files are then decompressed in chunks and filtered line by line to include only statistics for articles within the Wikipedia project. Each filtered chunk is then written to a parquet file. To query the election article page views, a manually compiled dictionary containing the names of the election articles in the included at the election dates is used. One SQL query per country per file is made using an in-memory duckDB database using python code in the following manner:

\begin{verbatim}
    temp = duckdb.query(f"""
    SELECT *
    FROM '{input_filepath}'
    WHERE  article_title = '{page_name}' AND wikicode = '{wikicode}'
\end{verbatim}

The results are stored in separate Pandas dataframes, with each row representing the hourly page views for the Wikipedia election page in a specific country. As all the steps are performed concurrently, the next step involves concatenating these dataframes. Moreover, the data is stacked to transform each row into a daily count, and then pivoted so that each column corresponds to a country and contains a time series of page views. Additionally, a new column is added to contain page views that are normalized relative to the defined time span, ensuring that the time series are comparable.








