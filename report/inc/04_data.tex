\chapter{Data}
\label{sec:data}
This project aims to analyze the relationship between Wikipedia page views and election outcomes in the EU elections of 2009, 2014, and 2019. To achieve this, two key metrics are required: Wikipedia page views for the relevant Wikipedia pages on the elections in the included countries, and the election turnout data. To obtain these metrics, the project retrieves data from two sources: Wikipedia Page View Dumps and the official EU website containing election results. The Wikipedia page views are accessed through downloadable files, as the API Wikimedia provides for this purpose only provides data from 2015 onwards. These files contain page views from all Wikimedia projects in all languages and can be quite large. In 2019, for example, they range from 400-500 MB uncompressed to around 4 GB uncompressed. Another challenge is that the data before December 2011 has a different format both in content and on the time basis the files exist at. This necessitates separate data processing steps to handle these differences.

To process the page view dump files, the project uses the Python framework and cloud computing provider modal.com. Intermediate results are stored on a remote S3 server.  Files corresponding to 14 days before and after the election date are downloaded and processed concurrently, handling 100 files at a time. The time span of 28 days is chosen to limit the amount of data to be processed as it would have been too resource and time-consuming for the scope of this project otherwise. The files are then decompressed in chunks and filtered line by line to include only statistics for articles within the Wikipedia project. Each filtered chunk is then written to a parquet file. To query the election article page views, a manually compiled dictionary containing the names of the election articles in the included at the election dates is used. One SQL query per country per file is made using an in-memory duckDB database using python code in the following manner:

\begin{verbatim}
    temp = duckdb.query(f"""
    SELECT *
    FROM '{input_filepath}'
    WHERE  contains(article_title, '{page_name}') AND wikicode = '{wikicode}'
\end{verbatim}

The results are stored in individual Pandas dataframes, where each row represents the hourly page views for the Wikipedia election page in a specific country. Since all the steps are performed concurrently, the final step involves concatenating these dataframes.




